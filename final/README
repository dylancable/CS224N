CS224n final project
Yaron Friedman and Issao Fujiwara

Our project consists of these main parts:

- Data crawling.
The relevant set of commands we used are in crawled/crawl.sh. Executing
that command will fire off several recursive wgets. These step took about
~4 hours to gather the data that we finally used


- Mining relevant English text form articles.

The script we used to extract relevant English text from the crawled
corpora is at py/bparser.py, which depends on the Beautiful Soup HTML
parser library, included in the same directory.

Assuming you've ran all the crawling described in crawl.sh, and all the
crawled directories are in the current directory, the following command
extracts the English text we used from the raw crawled html:

cat sources | ./run_parser.sh


- Creating training and tuning data for the language models.

This just involved splitting the input files in a train set with 90% of
the lines and a tune set with the other 10% of the lines. We used the
following scripts:
./create_train_and_tune_data.sh  # Creates the 'splits' file
cat splits | ./partition.sh  # Creates .tune and .train files.


- Training language models using the corpora generated above and evaluating
every other corpora with it.

We reused the language models we built in PA1, with some modifications to
make the language model trained with each corpora evaluate the perplexity
of each other corpora. The java code is all under the java/ directory.

First, build our Java code:
cd java
ant
mkdir models
./run_all_models.sh

This puts all the computed perplexity values in csv format in files in the
directory java/models. We have copied all our computed results to the
directory results/

In order to get a single file that can be read by matlab, we did:
./combine_model_results.sh


- Performing clustering analysis using MATLAB

We used MATLAB to perform most of our clustering analysis.
Here is a sample of commands:
> perp_matrix = textread('results/combined_values_tab.csv')
> labels = textread('results/rows', '%s')
> clustergram(perp_matrix, 'RowLabels', labels, 'ColumnLabels', labels)

We used the same method with differnt rows and columns subsets

Altenatively, you can load the mat file at results/state.mat and graph any
of the cg_* clustegrams by doing:
plot(cg_news)


- Identifying unigrams with most varying frequencies across all corpora

We computed the unigram frequencies using compute_counts.sh and then
used find_important_words.py to find among the top occuring words
the ones that have the most difference in frequency across all corpora.

Finally, we took this set of words:
foundation
congress
liberty
university
immigration
books
illegal
fought
taxes
college
research
mother
voters
medical

And perform the same clustering analysis in matlab using the frequency
of each of these words for each of the corpora.
